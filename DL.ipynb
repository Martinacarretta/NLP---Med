{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Deep Learning Method**"
      ],
      "metadata": {
        "id": "Wx_UHYJdpgit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group 7:\n",
        "- Martina Carretta\n",
        "- Meritxell Carvajal\n",
        "- Mariona Pla\n",
        "- Ares Sellart"
      ],
      "metadata": {
        "id": "23Ce-E1Ppowv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "52PoYKOBpfkY"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet medspacy\n",
        "!pip install --quiet spacy nltk\n",
        "#!python -m spacy download ca_core_news_sm > /dev/null 2>&1\n",
        "#!python -m spacy download es_core_news_sm > /dev/null 2>&1\n",
        "#!python -m spacy download es_core_news_md > /dev/null 2>&1\n",
        "!python -m spacy download es_core_news_lg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "import medspacy\n",
        "from spacy.tokens import Token\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, Bidirectional\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "KgVHbhkWV_YI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the data"
      ],
      "metadata": {
        "id": "P0VvaHXTpsO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Martinacarretta/githubTest.git\n",
        "jsons = open('/content/githubTest/negacio_train_v2024.json')\n",
        "json_string = jsons.read()\n",
        "json_object = json.loads(json_string)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmz2_G6kMtIC",
        "outputId": "5b407c47-7409-4135-9204-dcb9e2e28662"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'githubTest' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BTX4JAa1RKWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_char_to_token(tokens, text, char_indices):\n",
        "    start_char_index, end_char_index = char_indices\n",
        "    token_start_index = next(i for i, token in enumerate(tokens) if token.idx >= start_char_index)\n",
        "    token_end_index = next(i for i, token in enumerate(tokens) if token.idx >= end_char_index)\n",
        "    return token_start_index, token_end_index"
      ],
      "metadata": {
        "id": "ZakyVkM4ToXC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_es = spacy.load('es_core_news_lg') #Outside the for loop as it can be used as the same variable for each entry\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for entry in json_object:\n",
        "    text = entry.get('data')['text']\n",
        "    doc = nlp_es(text)\n",
        "\n",
        "    # Create vectors of true labels\n",
        "    true = np.zeros(len(doc), dtype=int)\n",
        "\n",
        "    for prediction in entry.get('predictions', []):\n",
        "        for label_data in prediction['result']:\n",
        "          label_value = label_data['value']\n",
        "          labels = label_value['labels']\n",
        "          start_index = label_value['start']\n",
        "          end_index = label_value['end']\n",
        "          text2 = text[start_index:end_index]  # Extract text based on start and end indexes\n",
        "\n",
        "          # Add words to corresponding sets based on labels\n",
        "          for label in labels:\n",
        "              if label == \"NEG\":\n",
        "                  start, end = convert_char_to_token(doc, text, (start_index, end_index)) # get index of negation in token form\n",
        "                  true[start:end] = 1\n",
        "\n",
        "              if label == \"UNC\":\n",
        "                  start, end = convert_char_to_token(doc, text, (start_index, end_index)) # get index of uncertainty in token form\n",
        "                  true[start:end] = 2\n",
        "              if label == \"NSCO\":\n",
        "                  start, end = convert_char_to_token(doc, text, (start_index, end_index)) # get index of negation scope in token form\n",
        "                  true[start:end] = 3\n",
        "              if label == \"USCO\":\n",
        "                  start, end = convert_char_to_token(doc, text, (start_index, end_index)) # get index of uncertainty scope in token form\n",
        "                  true[start:end] = 4\n",
        "\n",
        "\n",
        "    tokens_list = [token.text.lower() for token in doc] # Doc has object type, to work with word embeddings, we need a list of tokens. The lower() is to ensure consistency\n",
        "\n",
        "    # Generate the feature vectors and labels\n",
        "    list_for_dictionaries = [] # to append every word in a same doc\n",
        "    for i, token in enumerate(tokens_list):\n",
        "        x_vec = {'word': token , 'POS': doc[i].pos_} # For every token\n",
        "        list_for_dictionaries.append(x_vec)\n",
        "\n",
        "    X.append(list_for_dictionaries) # Here we should have a list for each doc with nested dictionaries for each word\n",
        "    true = list(true)\n",
        "    y.append(true)\n",
        "\n",
        "y = [[str(element) for element in sequence] for sequence in y]\n"
      ],
      "metadata": {
        "id": "4sx5IGnqSiLL"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding:"
      ],
      "metadata": {
        "id": "9N90-FxEUq7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mim_doc_length = min(len(doc) for doc in X)\n",
        "mim_doc_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkXHE3rbxq2K",
        "outputId": "34fc8f71-270a-46d2-e134-de21c5157eb0"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_length = sum(len(doc) for doc in X)\n",
        "average = total_length / len(X)\n",
        "average"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXKteuWgyAV2",
        "outputId": "e17fc03d-0027-4415-fdf7-0b634a1e6eeb"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "992.007874015748"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_doc_length = max(len(doc) for doc in X)\n",
        "\n",
        "padded_X = []\n",
        "for doc in X:\n",
        "    padded_doc = doc + [{'word': '<PAD>', 'POS': '<PAD>'}] * (max_doc_length - len(doc))\n",
        "    padded_X.append(padded_doc)\n",
        "\n",
        "padded_y = pad_sequences(y, maxlen=max_doc_length, padding='post', value=5)"
      ],
      "metadata": {
        "id": "XdEx9eBxUqbt"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = set([item['word'] for doc in padded_X for item in doc])\n",
        "all_pos = set([item['POS'] for doc in padded_X for item in doc])\n",
        "\n",
        "word_index = {word: i+1 for i, word in enumerate(all_words)}\n",
        "pos_index = {pos: i+1 for i, pos in enumerate(all_pos)}\n",
        "\n",
        "# Convert to sequences of indices\n",
        "X_word_sequences = [[word_index[token['word']] for token in doc] for doc in padded_X]\n",
        "X_pos_sequences = [[pos_index[token['POS']] for token in doc] for doc in padded_X]\n",
        "\n",
        "# Combine word and POS sequences if necessary\n",
        "X_combined_sequences = np.array([np.array([X_word_sequences[i], X_pos_sequences[i]]).T for i in range(len(X_word_sequences))])"
      ],
      "metadata": {
        "id": "nLeNPaXyZgh1"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_combined_sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpDYb-1_dvzu",
        "outputId": "b71945bd-d02d-4afe-cbab-85fb4ba079b7"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(254, 4174, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reshape and concatenate to fit the input requirements of LSTM"
      ],
      "metadata": {
        "id": "kMB89kg_epil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, TimeDistributed, Dense, Concatenate\n",
        "\n",
        "\n",
        "word_input = Input(shape=(max_doc_length,), dtype='int32', name='word_input')\n",
        "pos_input = Input(shape=(max_doc_length,), dtype='int32', name='pos_input')\n",
        "\n",
        "# EMbedding:\n",
        "word_embedding = Embedding(input_dim=len(word_index) + 1, output_dim=128, input_length=max_doc_length)(word_input)\n",
        "pos_embedding = Embedding(input_dim=len(pos_index) + 1, output_dim=32, input_length=max_doc_length)(pos_input)\n",
        "\n",
        "# Concatenate:\n",
        "combined = Concatenate()([word_embedding, pos_embedding])\n",
        "# The resulting tensor has a shape of (batch_size, max_doc_length, 160) (128 + 32), where 160 is the combined dimension of word and POS embeddings."
      ],
      "metadata": {
        "id": "SajAePocenx4"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: LSTM"
      ],
      "metadata": {
        "id": "VVKDxYH3XfMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "RPJ3uY0YYJro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bi_lstm = Bidirectional(LSTM(units=64, return_sequences=True))(combined)\n",
        "output = TimeDistributed(Dense(6, activation='softmax'))(bi_lstm) ############ % classes\n",
        "model = Model(inputs=[word_input, pos_input], outputs=[output])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1GKgYkpeNGX",
        "outputId": "3eff8c11-d3ab-4597-9e1f-83a10674e03e"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " word_input (InputLayer)     [(None, 4174)]               0         []                            \n",
            "                                                                                                  \n",
            " pos_input (InputLayer)      [(None, 4174)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding_9 (Embedding)     (None, 4174, 128)            2613888   ['word_input[0][0]']          \n",
            "                                                                                                  \n",
            " embedding_10 (Embedding)    (None, 4174, 32)             640       ['pos_input[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate  (None, 4174, 160)            0         ['embedding_9[0][0]',         \n",
            " )                                                                   'embedding_10[0][0]']        \n",
            "                                                                                                  \n",
            " bidirectional_5 (Bidirecti  (None, 4174, 128)            115200    ['concatenate_4[0][0]']       \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " time_distributed_5 (TimeDi  (None, 4174, 6)              774       ['bidirectional_5[0][0]']     \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2730502 (10.42 MB)\n",
            "Trainable params: 2730502 (10.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "vvKPMCxSZ-Kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_words, X_test_words, X_train_pos, X_test_pos, y_train, y_test = train_test_split(X_word_sequences, X_pos_sequences, padded_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit([X_train_words, X_train_pos], y_train, batch_size=32, epochs=5, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "UP8enx22ZoxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "I4VxCV-ggokM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict([np.array(X_test_words), np.array(X_test_pos)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yJEIfm7geq-",
        "outputId": "d24c22bd-56d2-4650-f381-13b51bd07577"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 5s 2s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipHEmKl7gqd4",
        "outputId": "350f0c0f-0830-47dc-9df2-acddf608d3f8"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.17059693, 0.16356897, 0.16624984, 0.16843835, 0.16487534,\n",
              "         0.16627055],\n",
              "        [0.16645446, 0.16411874, 0.16865493, 0.167405  , 0.16696428,\n",
              "         0.16640252],\n",
              "        [0.16732156, 0.16473852, 0.16895357, 0.16697778, 0.16605714,\n",
              "         0.16595136],\n",
              "        ...,\n",
              "        [0.16419163, 0.16830443, 0.16175117, 0.172498  , 0.1703677 ,\n",
              "         0.16288705],\n",
              "        [0.16443886, 0.16817582, 0.16169311, 0.17293167, 0.16984224,\n",
              "         0.1629183 ],\n",
              "        [0.16496176, 0.16800125, 0.16160154, 0.17336658, 0.16913894,\n",
              "         0.16293   ]],\n",
              "\n",
              "       [[0.17059587, 0.16356929, 0.16625544, 0.16843696, 0.16487314,\n",
              "         0.16626933],\n",
              "        [0.16645326, 0.164119  , 0.16866173, 0.16740292, 0.16696203,\n",
              "         0.16640101],\n",
              "        [0.16732025, 0.16473892, 0.16896151, 0.16697483, 0.16605482,\n",
              "         0.16594976],\n",
              "        ...,\n",
              "        [0.16419163, 0.16830443, 0.16175117, 0.172498  , 0.1703677 ,\n",
              "         0.16288705],\n",
              "        [0.16443886, 0.16817582, 0.16169311, 0.17293167, 0.16984224,\n",
              "         0.1629183 ],\n",
              "        [0.16496176, 0.16800125, 0.16160154, 0.17336658, 0.16913894,\n",
              "         0.16293   ]],\n",
              "\n",
              "       [[0.17058901, 0.1635789 , 0.16625124, 0.16844326, 0.16487412,\n",
              "         0.1662635 ],\n",
              "        [0.16644539, 0.16413103, 0.16865648, 0.1674098 , 0.16696283,\n",
              "         0.16639453],\n",
              "        [0.16731077, 0.1647535 , 0.16895536, 0.16698264, 0.16605534,\n",
              "         0.1659423 ],\n",
              "        ...,\n",
              "        [0.16419163, 0.16830443, 0.16175117, 0.172498  , 0.1703677 ,\n",
              "         0.16288705],\n",
              "        [0.16443886, 0.16817582, 0.16169311, 0.17293167, 0.16984224,\n",
              "         0.1629183 ],\n",
              "        [0.16496176, 0.16800125, 0.16160153, 0.17336658, 0.16913894,\n",
              "         0.16293   ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.1705836 , 0.16358179, 0.1662564 , 0.16844718, 0.16487263,\n",
              "         0.1662584 ],\n",
              "        [0.16643928, 0.16413434, 0.16866258, 0.16741364, 0.16696171,\n",
              "         0.16638848],\n",
              "        [0.16730337, 0.16475745, 0.16896263, 0.16698676, 0.16605447,\n",
              "         0.1659352 ],\n",
              "        ...,\n",
              "        [0.16419163, 0.16830443, 0.16175117, 0.172498  , 0.1703677 ,\n",
              "         0.16288705],\n",
              "        [0.16443886, 0.16817582, 0.16169311, 0.17293167, 0.16984224,\n",
              "         0.1629183 ],\n",
              "        [0.16496176, 0.16800125, 0.16160154, 0.17336658, 0.16913894,\n",
              "         0.16293   ]],\n",
              "\n",
              "       [[0.17059416, 0.16357411, 0.1662527 , 0.16843885, 0.16487272,\n",
              "         0.16626751],\n",
              "        [0.16645096, 0.16412549, 0.16865791, 0.16740501, 0.16696176,\n",
              "         0.16639896],\n",
              "        [0.16731708, 0.16474713, 0.16895647, 0.16697724, 0.1660548 ,\n",
              "         0.16594729],\n",
              "        ...,\n",
              "        [0.16419163, 0.16830443, 0.16175117, 0.172498  , 0.1703677 ,\n",
              "         0.16288705],\n",
              "        [0.16443886, 0.16817582, 0.16169311, 0.17293167, 0.16984224,\n",
              "         0.1629183 ],\n",
              "        [0.16496176, 0.16800125, 0.16160154, 0.17336658, 0.16913894,\n",
              "         0.16293   ]],\n",
              "\n",
              "       [[0.17058505, 0.16358   , 0.16626056, 0.1684458 , 0.16486366,\n",
              "         0.1662649 ],\n",
              "        [0.16644055, 0.16413239, 0.16866748, 0.16741285, 0.16695066,\n",
              "         0.16639619],\n",
              "        [0.16730458, 0.16475494, 0.16896819, 0.16698688, 0.16604102,\n",
              "         0.16594428],\n",
              "        ...,\n",
              "        [0.16419163, 0.16830443, 0.16175117, 0.172498  , 0.1703677 ,\n",
              "         0.16288705],\n",
              "        [0.16443887, 0.16817583, 0.16169313, 0.17293169, 0.16984226,\n",
              "         0.16291831],\n",
              "        [0.16496174, 0.16800123, 0.16160153, 0.17336656, 0.16913892,\n",
              "         0.16293   ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = np.argmax(predictions, axis=-1)"
      ],
      "metadata": {
        "id": "fhODhjGsmm67"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels[18]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY4J2ZxdnQ5M",
        "outputId": "988f8a86-69fd-469e-a562-9097219fa82c"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 2, ..., 3, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "cr = classification_report(y_test[20], predicted_labels[20])\n",
        "print(cr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PFbs7rasrCs",
        "outputId": "0a63a9d2-d561-4975-a908-3aae57df470c"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.06      0.12      2050\n",
            "           1       0.01      0.08      0.02        25\n",
            "           2       0.01      0.33      0.01         9\n",
            "           3       0.06      0.29      0.10        83\n",
            "           4       0.00      0.06      0.00        31\n",
            "           5       0.00      0.00      0.00      1976\n",
            "\n",
            "    accuracy                           0.04      4174\n",
            "   macro avg       0.17      0.14      0.04      4174\n",
            "weighted avg       0.46      0.04      0.06      4174\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_ones(binary_vector):\n",
        "    count = 0\n",
        "    for bit in binary_vector:\n",
        "        if bit == 1:\n",
        "            count += 1\n",
        "    return count\n",
        "counter = 0\n",
        "for llista in y_test:\n",
        "  counter += count_ones(llista)\n",
        "\n",
        "print(counter)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okXmps7pngP0",
        "outputId": "4a233241-08be-45e9-8059-1000b16b9a48"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kihd1kpfreUV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}